{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.models import model_from_json\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model, name):\n",
    "    print('SAVING model: {0}'.format(name))    \n",
    "    json_string = model.to_json()\n",
    "    architecture = name+'_architecture.json' \n",
    "    weights = name+'_weights.h5'\n",
    "    open(architecture, 'w').write(json_string)\n",
    "    model.save_weights(weights)\n",
    "\n",
    "\n",
    "def retrieve_model(name, weights=True):\n",
    "    print('RETRIEVING model: {0}'.format(name))\n",
    "    architecture = name + '_architecture.json' \n",
    "    model_saved = model_from_json(open(architecture).read())\n",
    "    \n",
    "    if weights:\n",
    "        weights = name+'_weights.h5'    \n",
    "        model_saved.load_weights(weights)\n",
    "    return model_saved\n",
    "\n",
    "\n",
    "\n",
    "def train_by_cluster(cluster,cluster_users,hps):\n",
    "    count = len(cluster_users)    \n",
    "    if(count < 50):\n",
    "        return\n",
    "    print('Training model: {0}:: users: {1}'.format(cluster,len(cluster_users)))\n",
    "    train_files = get_files(cluster_users,hps.train_file)\n",
    "    train_X, train_y  = get_cluster_data(train_files,hps)\n",
    "    print('TRAIN:: ',train_X.shape,train_y.shape)\n",
    "\n",
    "    epochs = hps.epochs \n",
    "    if(hps.epochs > 10):\n",
    "        count = len(cluster_users)\n",
    "        if(count > 400):\n",
    "            epochs = 100\n",
    "        elif(count  > 200):\n",
    "            epochs = 350\n",
    "        elif(count  > 75):\n",
    "            epochs = 400\n",
    "        \n",
    "    \n",
    "    # design network\n",
    "    model = get_lstm(hps,train_X,hps.dropout[0])\n",
    "\n",
    "    \n",
    "    validate_files = get_files(cluster_users,hps.validate_file)\n",
    "    if(validate_files):\n",
    "        validate_X, validate_y  = get_cluster_data(validate_files,hps)\n",
    "        print('VALIDATE:: ',validate_X.shape,validate_y.shape)    \n",
    "        \n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=hps.batch_size, \n",
    "                            validation_data=(validate_X, validate_y), \n",
    "                            verbose=hps.verbose_level, \n",
    "                            shuffle=False)\n",
    "        \n",
    "        if(hps.plot_eval):\n",
    "            pyplot.plot(history.history['val_loss'], label='test')\n",
    "        \n",
    "    else:        \n",
    "        print('VALIDATE:: NONE')    \n",
    "        validate_X, validate_y = [],[]\n",
    "        \n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, \n",
    "                            epochs=epochs, \n",
    "                            batch_size=hps.batch_size, \n",
    "                            verbose=hps.verbose_level, \n",
    "                            shuffle=False)\n",
    "        \n",
    "    # plot history\n",
    "    if(hps.plot_eval):\n",
    "        pyplot.plot(history.history['loss'], label='train')\n",
    "        pyplot.legend()\n",
    "        pyplot.show()   \n",
    "    \n",
    "    model_name = 'model_{0}'.format(cluster)\n",
    "    model_file = hps.model_dir+model_name\n",
    "    save_model(model,model_file)\n",
    "\n",
    "    \n",
    "def test_by_cluster(cluster,cluster_users,hps):\n",
    "    \n",
    "    count = len(cluster_users)    \n",
    "    if(count < 50):\n",
    "        return None,None,None\n",
    "    print('Testing cluster: {0}:: users: {1}'.format(cluster,len(cluster_users)))\n",
    "    model_name = 'model_{0}'.format(cluster)\n",
    "    model_file = hps.model_dir+model_name\n",
    "    model = retrieve_model(model_file)\n",
    "    \n",
    "    \n",
    "    test_files = get_files(cluster_users,hps.test_file)\n",
    "    if(test_files):\n",
    "        test_X, test_y  = get_cluster_data(test_files,hps)\n",
    "        print('TEST:: ',test_X.shape,test_y.shape)    \n",
    "    \n",
    "        # make a prediction\n",
    "        yhat = model.predict(test_X)\n",
    "        test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "        # invert scaling for forecast\n",
    "        inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "        inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "        inv_y = inv_y[:,0]    \n",
    "        get_performace(inv_y,inv_yhat,test_X.shape[0],hps)\n",
    "        return inv_y,inv_yhat,test_X.shape[0]\n",
    "    else:\n",
    "        return None,None,None\n",
    "    \n",
    "\n",
    "def train(hps):\n",
    "    try:\n",
    "        shutil.rmtree(hps.model_dir) \n",
    "        os.mkdir(hps.model_dir)\n",
    "    except OSError as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    users = get_clusters(hps)\n",
    "    i = 0\n",
    "    for val in users:\n",
    "        train_by_cluster(i,val,hps)\n",
    "        i += 1\n",
    "    return users    \n",
    "\n",
    "\n",
    "def test(users,hps):\n",
    "#    users = get_clusters(hps)\n",
    "    i = 0\n",
    "    examples = 0\n",
    "    actual = []\n",
    "    pred = []\n",
    "    for val in users:\n",
    "        y, yhat,samples = test_by_cluster(i,val,hps)\n",
    "        if(y is not None):\n",
    "            examples = examples + samples\n",
    "            actual = np.concatenate((actual,y),axis=None)\n",
    "            pred = np.concatenate((pred,yhat),axis=None)\n",
    "        i += 1\n",
    "#    return actual,pred,examples    \n",
    "    get_performace(actual,pred,examples,hps)\n",
    "        \n",
    "        \n",
    "def get_user_file(user_id,file_name):\n",
    "    u_id = str(int(user_id)).zfill(6)\n",
    "    file_id = 'user_{0}'.format(u_id)\n",
    "    file_path = file_name.format(file_id)\n",
    "    exists = os.path.isfile(file_path)\n",
    "    if exists:\n",
    "        return file_path\n",
    "    else:\n",
    "        return '' \n",
    "\n",
    "\n",
    "def get_files(cluster_users,file_path):\n",
    "    files = []\n",
    "    for uid in cluster_users:\n",
    "        fname = get_user_file(uid,file_path)\n",
    "        if(fname is not ''):\n",
    "            files.append(fname)\n",
    "    return files\n",
    "\n",
    "    \n",
    "def get_clusters(hps):\n",
    "    columns = ['user','gender','age','country','registered',\n",
    "                'artist','track','total_sessions','avg_session_length']\n",
    "    complete_files = glob.glob(hps.data_file)\n",
    "    dataset = pd.concat((pd.read_csv(f,names=columns,sep='\\t') for f in complete_files))\n",
    "    values = dataset.values\n",
    "    Xpair = values[:,(0,8)]\n",
    "    km = KMeans (n_clusters=hps.clusters, init='k-means++')\n",
    "    clstrs = km.fit (Xpair)\n",
    "    user_clusters = km.predict(Xpair) \n",
    "    users = []\n",
    "    for i in range(5):\n",
    "        x = []\n",
    "        a = np.where(user_clusters==i)\n",
    "        arr = Xpair[a]\n",
    "        for usr,val in arr:\n",
    "            x.append(usr)\n",
    "        users.append(x)    \n",
    "    return users        \n",
    "\n",
    "\n",
    "def experiment(hps,dropout):\n",
    "    train_X, train_y  = get_data(hps.train_file,hps)\n",
    "    print('TRAIN:: ',train_X.shape,train_y.shape)\n",
    "\n",
    "\n",
    "    validate_X, validate_y  = get_data(hps.validate_file,hps)\n",
    "    print('VALIDATE:: ',validate_X.shape,validate_y.shape)    \n",
    "    \n",
    "    \n",
    "    # design network\n",
    "    if(hps.model_lstm):\n",
    "        if(hps.layered):\n",
    "            model = get_layered_lstm(hps,train_X,dropout)\n",
    "        else:\n",
    "            model = get_lstm(hps,train_X,dropout)\n",
    "    else:    \n",
    "        model = get_gru(hps,train_X,dropout)\n",
    "        \n",
    "    # fit network\n",
    "    history = model.fit(train_X, train_y, \n",
    "                        epochs=hps.epochs, \n",
    "                        batch_size=hps.batch_size, \n",
    "                        validation_data=(validate_X, validate_y), \n",
    "                        verbose=hps.verbose_level, \n",
    "                        shuffle=False)\n",
    "    # plot history\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='test')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()   \n",
    "    \n",
    "    \n",
    "    test_X, test_y  = get_data(hps.test_file,hps)\n",
    "    print('TEST:: ',test_X.shape,test_y.shape)    \n",
    "    \n",
    "    # make a prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "    inv_y = inv_y[:,0]\n",
    "    \n",
    "    return get_performace(inv_y,inv_yhat,test_X.shape[0],hps)\n",
    "\n",
    "\n",
    "\n",
    "def get_baseline_data(file_name):\n",
    "    columns = ['user','current','start','session_id',\n",
    "               'prev_session_length','avg_session_length',\n",
    "               'gender','age','country','registered',\n",
    "               'track_duration','times_played','artist','track','session_length']\n",
    "\n",
    "    complete_files = glob.glob(file_name)\n",
    "    dataset = pd.concat((pd.read_csv(f,names=columns,sep='\\t') for f in complete_files))\n",
    "#     df_perc = np.percentile(dataset.session_length, [99.5])\n",
    "#     dataset =  dataset[dataset.session_length < df_perc[0]]\n",
    "#     dataset =  dataset[dataset.prev_session_length < df_perc[0]]\n",
    "    print('DATA:: ',dataset.shape)\n",
    "\n",
    "    \n",
    "    return dataset   \n",
    "\n",
    "\n",
    "\n",
    "def test_baseline(train,file_name):\n",
    "   \n",
    "    test  = get_baseline_data(file_name)\n",
    "    train['session_length'] = train['session_length'].astype('float64') \n",
    "    _train = train.groupby(['user'])['session_length'].mean().to_dict()\n",
    "    \n",
    "    inv_yhat = []\n",
    "    inv_y = []\n",
    "    # make a prediction\n",
    "    for row in tqdm(test.iterrows(),total=test.shape[0]):\n",
    "        try:\n",
    "            user = row[1]['user']\n",
    "            pred = _train[user]\n",
    "            inv_yhat.append(pred)\n",
    "            inv_y.append(float(row[1]['session_length']))\n",
    "        except Exception as e:    \n",
    "            pass\n",
    "\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    \n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)\n",
    "    print('Test MAE: %.3f' % mae)\n",
    "    \n",
    "    \n",
    "def get_cluster_data(file_names,hps):\n",
    "    columns = ['start','user','session_id','gender','age','country','registered',\n",
    "               'prev_session_length','avg_session_length','session_length']\n",
    "    \n",
    "#     columns = ['user','current','start','session_id',\n",
    "#                'prev_session_length','avg_session_length',\n",
    "#                'gender','age','country','registered',\n",
    "#                'track_duration','times_played','artist','track','session_length']\n",
    "    dataset = pd.concat((pd.read_csv(f,names=columns,sep='\\t') for f in file_names))\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    values = dataset.values\n",
    "    X = values[:,:-1]\n",
    "    y = values[:,-1]    \n",
    "    \n",
    "    #3D - samples,timesteps,features\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def get_data(file_name,hps):\n",
    "    columns = ['user','current','start','session_id',\n",
    "               'prev_session_length','avg_session_length',\n",
    "               'gender','age','country','registered',\n",
    "               'track_duration','times_played','artist','track','session_length']\n",
    "    complete_files = glob.glob(file_name)\n",
    "    dataset = pd.concat((pd.read_csv(f,names=columns,sep='\\t') for f in complete_files))\n",
    "    \n",
    "    if(hps.filter_outliers):\n",
    "        df_perc = np.percentile(dataset.session_length, [hps.upper_limit])\n",
    "        dataset =  dataset[dataset.session_length < df_perc[0]]\n",
    "        dataset =  dataset[dataset.prev_session_length < df_perc[0]]\n",
    "        \n",
    "#         df_perc = np.percentile(dataset.session_length, [hps.lower_limit])\n",
    "#         dataset =  dataset[dataset.session_length > df_perc[0]]\n",
    "#         dataset =  dataset[dataset.prev_session_length > df_perc[0]]\n",
    "        \n",
    "    dataset = dataset.dropna()\n",
    "    #dataset = dataset.sort_values(by=['start'])  \n",
    "    \n",
    "    values = dataset.values\n",
    "    X = values[:,:-1]\n",
    "    y = values[:,-1]    \n",
    "    \n",
    "    #3D - samples,timesteps,features\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def get_layered_lstm(hps,train_X,dropout):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(hps.layer_dims, \n",
    "                   input_shape=(train_X.shape[1], \n",
    "                                train_X.shape[2]),\n",
    "                   return_sequences=True,\n",
    "                   dropout=dropout))\n",
    "    for i in range(hps.no_layers):\n",
    "        model.add(LSTM(hps.hidden_dim, \n",
    "                       dropout=dropout))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=hps.loss_func, \n",
    "                  optimizer=hps.optimizer)\n",
    "    return model\n",
    "\n",
    "def get_lstm(hps,train_X,dropout):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(hps.hidden_dim, \n",
    "                   input_shape=(train_X.shape[1], \n",
    "                                train_X.shape[2]),\n",
    "                   dropout=dropout))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=hps.loss_func, \n",
    "                  optimizer=hps.optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_gru(hps,train_X):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(hps.hidden_dim, \n",
    "                   input_shape=(train_X.shape[1], \n",
    "                                train_X.shape[2])))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=hps.loss_func, \n",
    "                  optimizer=hps.optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_performace(y,y_hat,samples,hps):\n",
    "    # calculate RMSE\n",
    "    rmse = sqrt(mean_squared_error(y, y_hat))\n",
    "    mae = mean_absolute_error(y, y_hat)\n",
    "    norm = mae/hps.baseline_mae\n",
    "    \n",
    "    print('METRICS :: RMSE: {0} ; MAE: {1} ; Normalized MAE: {2}'.format(rmse,mae,norm))    \n",
    "#     pyplot.figure()\n",
    "#     pyplot.plot(y, label='actual')\n",
    "#     pyplot.plot(y_hat, label='pred')\n",
    "#     pyplot.legend()\n",
    "#     pyplot.show()   \n",
    "    \n",
    "    return rmse,mae,norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
