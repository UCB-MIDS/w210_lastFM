{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"Survival Analysis EDA\"\n",
    "\n",
    "try:\n",
    "    sc.stop()\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "try:\n",
    "    sc and spark\n",
    "except (NameError, UnboundLocalError) as e:\n",
    "    import findspark\n",
    "    #findspark.init()\n",
    "    import pyspark\n",
    "    import pyspark.sql\n",
    "    \n",
    "    #sc = pyspark.SparkContext()\n",
    "    #spark = pyspark.sql.SparkSession(sc).builder.appName(APP_NAME).getOrCreate()\n",
    "    \n",
    "sc = SparkContext(conf = SparkConf() .set(\"spark.driver.maxResultSize\", \"60g\") .set(\"spark.sql.execution.arrow.enabled\", \"true\") .set('spark.sql.broadcastTimeout', 1000) .set('spark.local.dir', '/data_data/session_length/spark_tmp/') .set('spark.driver.memory', '60G') .set(\"spark.executor.instances\", \"10\") .set(\"spark.executor.cores\", 10) .set(\"spark.executor.memory\", \"20G\")).getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, DateType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max, lit, round, bround, pow, col, corr, lower, upper, avg, stddev, abs, log\n",
    "from pyspark.sql.functions import lit, trim, rtrim, rpad, trim, coalesce\n",
    "from pyspark.sql.functions import current_date, current_timestamp, date_add, date_sub, months_between, to_date\n",
    "from pyspark.sql.functions import udf, col, countDistinct, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, dense_rank, rank, expr\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RFormula, Bucketizer, QuantileDiscretizer\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, DecisionTreeClassifier, RandomForestClassifier, MultilayerPerceptronClassifier,NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor, GBTRegressor\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT MODEL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head new_model_df_11_12_2018.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 5000 new_model_df_11_27_2018.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Schema (here we assume 10 genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_Schema = StructType([StructField('userid', StringType(), True), \n",
    "                             StructField('session_id', IntegerType(), True), \n",
    "                             StructField('gender', StringType(), True), \n",
    "                             StructField('age', StringType(), True), \n",
    "                             StructField('country', StringType(), True), \n",
    "                             StructField('day_of_week', StringType(), True),\n",
    "                             StructField('time_of_day', StringType(), True), \n",
    "                             StructField('is_holiday', IntegerType(), True), \n",
    "                             StructField('time_since_epoch', DoubleType(), True), \n",
    "                             StructField('days_since_epoch', IntegerType(), True), \n",
    "                             StructField('absence_time', IntegerType(), True),\n",
    "                             StructField('genre_10_name', StringType(), True),\n",
    "                             StructField('genre_9_name', StringType(), True),\n",
    "                             StructField('genre_8_name', StringType(), True),\n",
    "                             StructField('genre_7_name', StringType(), True),\n",
    "                             StructField('genre_6_name', StringType(), True),\n",
    "                             StructField('genre_5_name', StringType(), True),\n",
    "                             StructField('genre_4_name', StringType(), True),\n",
    "                             StructField('genre_3_name', StringType(), True),\n",
    "                             StructField('genre_2_name', StringType(), True),\n",
    "                             StructField('genre_1_name', StringType(), True),\n",
    "                             StructField('undefined_genre_name', StringType(), True),\n",
    "                             StructField('genre_10_freq', DoubleType(), True),\n",
    "                             StructField('genre_9_freq', DoubleType(), True),\n",
    "                             StructField('genre_8_freq', DoubleType(), True),\n",
    "                             StructField('genre_7_freq', DoubleType(), True),\n",
    "                             StructField('genre_6_freq', DoubleType(), True),\n",
    "                             StructField('genre_5_freq', DoubleType(), True),\n",
    "                             StructField('genre_4_freq', DoubleType(), True),\n",
    "                             StructField('genre_3_freq', DoubleType(), True),\n",
    "                             StructField('genre_2_freq', DoubleType(), True),\n",
    "                             StructField('genre_1_freq', DoubleType(), True),\n",
    "                             StructField('undefined_genre_freq', DoubleType(), True), \n",
    "                             StructField('session_length', StringType(), True),\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the_model_df = spark.read.format('csv').options(sep=\"\\t\").schema(session_Schema).option('header',True).load('new_model_df_11_27_2018.tsv')\n",
    "\n",
    "#.drop('age','gender', 'genre_10','genre_9','genre_8','genre_7','genre_8','genre_7','genre_6','genre_5','genre_4','genre_3','genre_2','genre_1')\n",
    "\n",
    "the_model_df = spark.read.format('csv').options(sep='\\t').option(\"inferSchema\", \"true\").option('header',True).load('new_model_df_11_27_2018.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_df.show(500, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn categorical values into integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(the_model_df.columns)-set(['is_holiday','time_since_epoch','days_since_epoch','absence_time','genre_10_freq','genre_9_freq','genre_8_freq',\n",
    "                               'genre_7_freq','genre_6_freq','genre_5_freq','genre_4_freq','genre_3_freq','genre_2_freq','genre_1_freq','undefined_genre_freq',\n",
    "                               'session_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(the_model_df) for column in list(set(the_model_df.columns)-set(['is_holiday','time_since_epoch','days_since_epoch','absence_time','genre_10_freq','genre_9_freq','genre_8_freq','country_index',\n",
    "                                                                                                                                          'genre_7_freq','genre_6_freq','genre_5_freq','genre_4_freq','genre_3_freq','genre_2_freq','genre_1_freq','undefined_genre_freq','session_length',\n",
    "                                                                                                                                          ])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "the_model_df_ = pipeline.fit(the_model_df).transform(the_model_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "#the_model_df_.show(5)\n",
    "#spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_DF_ = the_model_df_.select('userid','session_id','age','gender','country','country_index','day_of_week','day_of_week_index','time_of_day','time_of_day_index','is_holiday','absence_time','days_since_epoch',\n",
    "                                    'genre_10_name','genre_9_name','genre_8_name', 'genre_7_name','genre_6_name','genre_5_name','genre_4_name','genre_3_name','genre_2_name','genre_1_name','undefined_genre_name',\n",
    "                                    'genre_10_freq','genre_9_freq','genre_8_freq', 'genre_7_freq','genre_6_freq','genre_5_freq','genre_4_freq','genre_3_freq','genre_2_freq','genre_1_freq','undefined_genre_freq','session_length')\n",
    "\n",
    "#the_model_DF_ = the_model_df_\n",
    "\n",
    "#the_model_DF_ = the_model_df_.select('userid','session_id','country','country_index','day_of_week','day_of_week_index','time_of_day','time_of_day_index','is_holiday','absence_time','days_since_epoch','session_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_DF_.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cols = the_model_DF_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#false_cols.append('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_df_columns_nullable(spark, df, column_list, nullable=True):\n",
    "    for struct_field in df.schema:\n",
    "        if struct_field.name in column_list:\n",
    "            struct_field.nullable = nullable\n",
    "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
    "    return df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_the_model_DF = set_df_columns_nullable(spark,the_model_DF_,false_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_DF = _the_model_DF\n",
    "#.withColumn(\"age_\", _the_model_DF[\"age\"].cast(\"double\"))\n",
    "#.drop('age').withColumnRenamed('age_','age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_DF.orderBy('userid','session_id').show(5000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_model_DF.select('userid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df = the_model_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.select('userid').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Distribution of Session Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = userSession_df.toPandas().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df2 = plot_df[['day_of_week','time_of_day','session_length']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(plot_df2, vars=[ 'time_of_day', 'session_length'],hue='time_of_day', palette='RdBu_r')\n",
    "g.map(plt.scatter, alpha=0.8)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(36, 15)\n",
    "sns.countplot(plot_df['session_length'])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df = userSession_df.withColumn('log_sess', round(log(\"session_length\"),2)).drop('session_length').withColumnRenamed('log_sess','session_length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = Window.partitionBy(\"userid\").orderBy(\"window\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "#(user_session_with_window_df.withColumn(\"cumSum\", sum(\"session_length\").over(w)).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_df = userSession_df.toPandas().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 15)\n",
    "sns.countplot(plot_log_df['session_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style='ticks'):\n",
    "    g = sns.factorplot(\"day_of_week\", \"session_length\", \"time_of_day\", data=plot_log_df, kind=\"box\", size=4, aspect=4)\n",
    "    g.set_axis_labels(\"Day of Week\", \"Session Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 12)\n",
    "sns.countplot(plot_df['day_of_week'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_tod_df = userSession_df.groupBy('time_of_day').agg(count(\"time_of_day\").alias(\"num_sessions\"))\n",
    "sess_tod_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeofday    = list([x[0] for x in pd.Series(sess_tod_df.select('time_of_day').collect())])\n",
    "num_sessions = list([y[0] for y in pd.Series(sess_tod_df.select('num_sessions').collect())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15,12)\n",
    "sns.countplot(plot_df['time_of_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_tod_sl_df = userSession_df.groupBy('time_of_day').agg(round(avg(\"session_length\"),2).alias(\"avg_sess_length\"))\n",
    "sess_tod_sl_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sess_len = list([z[0] for z in pd.Series(sess_tod_sl_df.select('avg_sess_length').collect())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sess_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(15,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.pairplot(plot_df2, hue='time_of_day', size=4.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"age\", \"session_length\", data=plot_log_df, hue='time_of_day', fit_reg=False, size=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.jointplot(\"session_length\", \"age\", data=plot_log_df.dropna(), kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(36, 24)\n",
    "sns.countplot(plot_log_df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(36, 24)\n",
    "sns.barplot(x=\"age\", y=\"session_length\", hue=\"gender\", data=plot_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 8)\n",
    "sns.barplot(x=\"day_of_week\", y=\"session_length\", hue=\"gender\", data=plot_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 8)\n",
    "sns.barplot(x=\"time_of_day\", y=\"session_length\", hue=\"gender\", data=plot_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age']\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(24, 12)\n",
    "#for col in cols:\n",
    "#    sns.kdeplot(plot_log_df[col], shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.kdeplot(plot_log_df.session_length[plot_log_df.gender=='M'], label='men', shade=True)\n",
    "#sns.kdeplot(plot_log_df.session_length[plot_log_df.gender=='W'], label='women', shade=True)\n",
    "#plt.xlabel('split_frac');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(36, 15)\n",
    "sns.countplot(plot_log_df['country'], order = plot_log_df['country'].value_counts().index)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(style='ticks'):\n",
    "    g = sns.factorplot(\"day_of_week\", \"session_length\", \"time_of_day\", data=plot_df, kind=\"box\", size=6, aspect=12)\n",
    "    g.set_axis_labels(\"Day of Week\", \"Session Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('white'):\n",
    "    g = sns.factorplot(\"day_of_week\", data=plot_log_df, aspect=8.0, kind='count',\n",
    "                       hue='time_of_day', size=6.0)\n",
    "    g.set_ylabels('Session Length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = userSession_df.select('session_length').rdd.map(lambda row: row.session_length).collect()\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data,columns=['session_length']).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deciles = list(np.percentile(data, np.arange(0, 100, 10)))\n",
    "deciles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Quantile Bucketized Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.select('userid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_windows = int(userSession_df.select('session_length').distinct().count())\n",
    "num_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets = 2.5*len(deciles)\n",
    "num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_length_bucketizer = QuantileDiscretizer(numBuckets=num_buckets, inputCol=\"session_length\", outputCol=\"decile\", relativeError=0.01, handleInvalid=\"error\")\n",
    "bucketizer = session_length_bucketizer.fit(userSession_df)\n",
    "user_session_with_window_bucketized_df = bucketizer.setHandleInvalid(\"skip\").transform(userSession_df)\n",
    "user_session_with_window_bucketized_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_df.select('userid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_dict = {1.0:\"<5_mins\",2.0:\"<5_mins\",3.0:\"<10_mins\",4.0:\"<20_mins\",5.0:\"<30_mins\",6.0:\"<40_mins\",7.0:\"<1_hour\",8.0:\"<1.5_hours\",9.0:\"<2_hours\",10.0:\">3_hours\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(decile):\n",
    "    try:\n",
    "        return decile_dict[decile]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_description(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_desc = udf(lambda bucket: get_description(bucket), returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df = user_session_with_window_bucketized_df.withColumn('description', get_desc('decile'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df.select('userid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"user_000002\".split(\"_\")[1].lstrip('0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_num(userid):\n",
    "    return str(userid.split(\"_\")[1].lstrip(\"0\"))\n",
    "get_user_num_udf = udf(lambda userid: get_user_num(userid), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df = user_session_with_window_bucketized_descr_df.withColumn('user_num', get_user_num_udf('userid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = None\n",
    "dev_data = None\n",
    "test_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = user_session_with_window_bucketized_descr_df\n",
    "\n",
    "\n",
    "#df.select($\"userid\",concat_ws(\":\",$\"genre_10\",$\"genre_9\",$\"genre_9\",$\"genre_8\",$\"genre_7\",$\"genre_6\",$\"genre_5\",$\"genre_4\",$\"genre_3\",$\"genre_2\",$\"genre_1\").as(\"new_col\")).groupBy($\"userid\").agg(collect_set($\"new_col\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGN USERS TO CLUSTERS BASED ON TIME_OF_DAY VARIANCES IN SESSION_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_session_with_window_bucketized_df.select('userid','session_id','country','time_of_day','day_of_week','is_holiday','session_length' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_var_df = user_df.select(\"userid\",\"time_of_day\",\"session_length\").groupBy('userid','time_of_day').agg(round(avg(\"session_length\"),1).alias('avg_time_of_day'),round(stddev(\"session_length\"),1).alias('std_time_of_day'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_var_df.orderBy('userid').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_var_df.pivot('time_of_day')\n",
    "\n",
    "\n",
    "#user_var_df.withColumn(\"fg\", F.row_number().over(Window().partitionBy('userid').groupBy('userid').pivot('fg').sum('avg_time_of_day', 'std_time_of_day').show()\n",
    "                                                 \n",
    "_user_var_df = user_var_df.select('userid','time_of_day','avg_time_of_day').na.fill(0.0).groupBy(\"userid\").pivot(\"time_of_day\").agg(max(\"avg_time_of_day\")).drop('Null').dropna()\n",
    "\n",
    "_user_var_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = _user_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_df = df.withColumn('total', round(sum(df[col] for col in df.columns[1:])))\n",
    "#my_schema = StructType([StructField(df.columns[0], StringType(), True)] + [StructField(col, DoubleType(), True) for col in df.columns[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_cols = new_df.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_bucketized_df.show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_buckets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.select('Afternoon').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = []\n",
    "\n",
    "for col in new_df_cols:\n",
    "    col_assembler = VectorAssembler(inputCols= [col], outputCol = col+\"_feature\")\n",
    "    col_scaler = StandardScaler(inputCol=col+\"_feature\", outputCol=col+\"_scaled\",withStd=True, withMean=True)\n",
    "    stages.append(col_assembler)\n",
    "    stages.append(col_scaler)\n",
    "    \n",
    "print(stages)\n",
    "col_pipeline = Pipeline(stages=stages)\n",
    "scaled = col_pipeline.fit(new_df)\n",
    "new_df_scaled = scaled.transform(new_df)\n",
    "type(new_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_cols = ['userid',\n",
    " 'Afternoon_scaled',\n",
    " 'Dawn_scaled',\n",
    " 'Evening_scaled',\n",
    " 'LateAfternoon_scaled',\n",
    " 'LateEvening_scaled',\n",
    " 'LateMorning_scaled',\n",
    " 'Lunch_scaled',\n",
    " 'Morning_scaled',\n",
    " 'Night_scaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled_ = new_df_scaled.select(scaled_cols)\n",
    "new_df_scaled_.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ 'round(df[\\\"'+col +\"\\\"],2).alias(\\\"new_\"+col+\"\\\")\" for col in scaled_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_scaled_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "\n",
    "def get_tod_cluster():\n",
    "    global user_tod_cluster_df\n",
    "    global ug_df\n",
    "    global stringIndexer\n",
    "    global ug_transformed\n",
    "    \n",
    "    ug_df = new_df_scaled_\n",
    "    \n",
    "    categoricalColumns = [\"userid\"]\n",
    "    cols = new_df_scaled_.columns\n",
    "    stages =[]\n",
    "    \n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer  = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol+\"classVec\"])\n",
    "        stages += [stringIndexer,encoder]\n",
    "\n",
    "    numericCols = ['Afternoon_scaled','Dawn_scaled', 'Evening_scaled', 'LateAfternoon_scaled','LateEvening_scaled','LateMorning_scaled', 'Lunch_scaled', 'Morning_scaled','Night_scaled']\n",
    "    assemblerInputs = [c + \"classVec\" for c in categoricalColumns]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    stages += [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(ug_df)\n",
    "    ug_df = pipelineModel.transform(ug_df)\n",
    "    selectedcols = cols + [\"features\"]\n",
    "    print(selectedcols)\n",
    "    ug_df = ug_df.select(selectedcols)\n",
    "\n",
    "\n",
    "    ug_df.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tod_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=10, seed=1)\n",
    "model = kmeans.fit(ug_df)\n",
    "\n",
    "ug_tr_df = model.transform(ug_df)\n",
    "    \n",
    "ug_tr_df = ug_tr_df.withColumnRenamed('userid','userid_tod')\n",
    "\n",
    "ug_tr_df.show(1000, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_user_var_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tod_cluster_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "\n",
    "def get_tod_cluster():\n",
    "    global user_tod_cluster_df\n",
    "    global ug_df\n",
    "    global stringIndexer\n",
    "    global ug_transformed\n",
    "    \n",
    "    ug_df = _user_var_df\n",
    "    \n",
    "    categoricalColumns = [\"userid\"]\n",
    "    cols = _user_var_df.columns\n",
    "    stages =[]\n",
    "    \n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer  = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol+\"classVec\"])\n",
    "        stages += [stringIndexer,encoder]\n",
    "\n",
    "    numericCols = [\"Afternoon\",\"Dawn\",\"Evening\",\"LateAfternoon\",\"LateEvening\",\"LateMorning\",\"Lunch\",\"Morning\",\"Night\"]\n",
    "    assemblerInputs = [c + \"classVec\" for c in categoricalColumns]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    stages += [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(ug_df)\n",
    "    ug_df = pipelineModel.transform(ug_df)\n",
    "    selectedcols = [\"features\"] + cols\n",
    "    ug_df = ug_df.select(selectedcols)\n",
    "\n",
    "\n",
    "    ug_df.select('userid','features').show(20, False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tod_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "kmeans = KMeans(k=10, seed=1)\n",
    "model = kmeans.fit(ug_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ug_tr_df = model.transform(ug_df)\n",
    "    \n",
    "ug_tr_df = ug_tr_df.select('userid','prediction').withColumnRenamed('userid','userid_tod')\n",
    "\n",
    "ug_tr_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugTR_df = ug_tr_df.withColumnRenamed('prediction','tod_cluster') \n",
    "ugTR_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGN USERS TO CLUSTERS BASED ON MUSIC TASTES (GENRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_genres_df = user_session_with_window_bucketized_df.select(\"userid\",\"genre_10_name\",\"genre_9_name\",\"genre_8_name\",\"genre_7_name\",\"genre_6_name\",\"genre_5_name\",\"genre_4_name\",\"genre_3_name\",\"genre_2_name\",\"genre_1_name\")\n",
    "user_genres_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usGN_df = user_genres_df.select('userid','genre_10_name').groupBy('userid','genre_10_name').count().orderBy(['userid','count'], ascending=[1, 0])\n",
    "usGN_df.show(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(df['userid']).orderBy(usGN_df['count'].desc())\n",
    "\n",
    "#us_GN_df = usGN_df.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 10).orderBy(['userid','rank'],ascending=[1,1]).select('userid','genre_10_name')\n",
    "\n",
    "#us_GN_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cluster_df = None\n",
    "assembler = None\n",
    "ug_df = None\n",
    "ug_transformed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "\n",
    "\n",
    "def get_top_genre_cluster():\n",
    "    global user_cluster_df\n",
    "    global ug_df\n",
    "    global assembler\n",
    "    global stringIndexer\n",
    "    global ug_transformed\n",
    "    global ug_gn_df\n",
    "    \n",
    "    ug_df = usGN_df.select('userid','genre_10_name')\n",
    "    \n",
    "    categoricalColumns = [\"userid\",\"genre_10_name\"]\n",
    "    cols = usGN_df.select('userid','genre_10_name').columns\n",
    "    stages =[]\n",
    "    \n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer  = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol+\"classVec\"])\n",
    "        stages += [stringIndexer,encoder]\n",
    "\n",
    "    numericCols = []\n",
    "    assemblerInputs = [c + \"classVec\" for c in categoricalColumns]\n",
    "    #assemblerInputs = numericCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    stages += [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(ug_df)\n",
    "    ug_df = pipelineModel.transform(ug_df)\n",
    "    selectedcols = [\"features\"] + cols\n",
    "    ug_gn_df = ug_df.select(selectedcols)\n",
    "\n",
    "\n",
    "    ug_gn_df.select('userid','features').show(20, False)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " get_top_genre_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(k=30, seed=1)\n",
    "model = kmeans.fit(ug_gn_df.select('features'))\n",
    "\n",
    "ug_gn_df = model.transform(ug_gn_df)\n",
    "    \n",
    "#ug_df = ug_gn_df.select('userid','prediction')\n",
    "\n",
    "ug_df = ug_gn_df\n",
    "\n",
    "ug_df.show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugGN_df = ug_df.withColumnRenamed('prediction','genre_cluster').withColumnRenamed('userid','userid_gn')\n",
    "ugGN_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugGN_df.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ugTR_df.orderBy('userid_tod').show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_the_df = user_session_with_window_bucketized_descr_df.select(\"userid\", \"country\",\"day_of_week\", \"time_of_day\",\"is_holiday\", \"absence_time\",\"days_since_epoch\",\"session_length\",\"decile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getTOD_cluster_id = udf(lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#t = ugTR_df.alias('t')\n",
    "g = ugGN_df.alias('g')\n",
    "d = _the_df.alias('d')\n",
    "#join_condition = [ (d.userid == t.userid_tod) ]\n",
    "join_condition = [ (d.userid == g.userid_gn) ]\n",
    "#bucketized_DF = d.join(t, join_condition, 'inner').drop('userid_tod')\n",
    "bucketized_DF = d.join(g, join_condition, 'inner').drop('userid_gn')\n",
    "\n",
    "bucketized_DF.show(2000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_session_with_window_bucketized_descr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "\n",
    "def get_train_dev_test_data():\n",
    "    print(\"==================================================================================================================\")\n",
    "    print(\"============================================     CREATE TRAIN/DEV/TEST SETS      =================================\")\n",
    "    print(\"==================================================================================================================\\n\\n\")\n",
    "    global train_data, dev_data, test_data\n",
    "    global user_session_with_window_df\n",
    "    \n",
    "    print(\"SESSION_DATA_WITH_WINDOW_BUCKETIZED_DESCR_DF.USERID.COUNT():\\t{}\".format(user_session_with_window_bucketized_descr_df.select(\"userid\").distinct().count()))\n",
    "    #session_data = user_session_with_window_bucketized_descr_df.select(\"userid\",\"gender\",\"age\",\"country\", \"day_of_week\", \"time_of_day\",\"is_holiday\",\"genre_10_name\",\"genre_9_name\",\"genre_8_name\",\"genre_7_name\",\"genre_6_name\",\"genre_5_name\",\"genre_4_name\",\"genre_3_name\",\"genre_2_name\",\"genre_1_name\",\"genre_10_freq\",\"genre_9_freq\",\"genre_8_freq\",\"genre_7_freq\",\"genre_6_freq\",\"genre_5_freq\",\"genre_4_freq\",\"genre_3_freq\",\"genre_2_freq\",\"genre_1_freq\",\"absence_time\",\"days_since_epoch\",\"session_length\",\"decile\").na.drop()\n",
    "    session_data = bucketized_DF\n",
    "    \n",
    "    print(\"SESSION_DATA.USERID.COUNT():\\t{}\".format(session_data.select(\"userid\").distinct().count()))\n",
    "    categoricalColumns = [\"userid\",\"country\",\"day_of_week\", \"time_of_day\"]\n",
    "    cols = session_data.columns\n",
    "    stages =[]\n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer  = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol+\"classVec\"])\n",
    "        stages += [stringIndexer,encoder]\n",
    "    label_stringIdx = StringIndexer(inputCol=\"decile\", outputCol=\"label\")\n",
    "    stages += [label_stringIdx]\n",
    "    numericCols = [\"is_holiday\",\"absence_time\",\"days_since_epoch\",\"session_length\",\"decile\",'genre_cluster']\n",
    "    assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    pipelineModel = pipeline.fit(session_data)\n",
    "    session_data = pipelineModel.transform(session_data)\n",
    "    selectedcols = cols + [\"features\",\"label\"] \n",
    "    session_data = session_data.select(selectedcols)\n",
    "    print(\"************************************       DATA BEFORE TRAIN/DEV/TEST SPLIT     *************************************\")\n",
    "    #session_data.show(5,False)\n",
    "    print(\"************************************           Splitting Train/Test data        *************************************\")\n",
    "    #train_data, dev_data, test_data = session_data.randomSplit([0.65, 0.2, 0.15])\n",
    "    \n",
    "    print(\"SESSION_DATA.COUNT():\\t{}\".format(session_data.count()))\n",
    "    print(\"SESSION_DATA.USERID.COUNT():\\t{}\".format(session_data.select(\"userid\").distinct().count()))\n",
    "\n",
    "    session_data_df = session_data.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"userid\"))).drop('decile')\n",
    "    train_data = session_data_df.where(\"rank <= .65\").drop(\"rank\")\n",
    "    #train_data.show()\n",
    "    dev_data = session_data_df.where((\"rank > .65\") and (\"rank < 0.799\")).drop(\"rank\")\n",
    "    #dev_data.show()\n",
    "    test_data = session_data_df.where(\"rank > .80\").drop(\"rank\")\n",
    "    #test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_dev_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE DL model with Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothings  = [0.2,0.4,0.6,0.8,1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnbModel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mnb = []\n",
    "\n",
    "def mnb_classifier(smoothing):\n",
    "    global results_mnb\n",
    "    global mnbModel\n",
    "    global lr_results\n",
    "    global train_data, dev_data, test_data\n",
    "    mnb = NaiveBayes(smoothing=smoothing, modelType=\"multinomial\")\n",
    "    #print(\"---------------------------------  Fitting Model to TRAIN  ---------------------------\")\n",
    "    mnbModel = mnb.fit(train_data)\n",
    "    #print(\"---------------------------------  Generating Predictions  ---------------------------\")\n",
    "    predictions = mnbModel.transform(dev_data)\n",
    "    selected = predictions.select(\"label\", \"prediction\", \"userid\",\"country\",\"is_holiday\",\"country\", \"day_of_week\", \"time_of_day\",\"absence_time\",\"days_since_epoch\",'genre_cluster')\n",
    "    evaluator = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol(\"prediction\")\n",
    "    #print(\"---------------------------------  Evaluating Model     ------------------------------\")\n",
    "    dev_result = evaluator.evaluate(mnbModel.transform(dev_data))\n",
    "    test_result = evaluator.evaluate(mnbModel.transform(test_data))\n",
    "    #result.select(\"features\",\"label\",\"prediction\").show(5,False)\n",
    "    mnb_results.update({smoothing:[dev_result,test_result]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_mnb_classifier():\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"############################     MULTINOMIAL NAIVE BAYES CLASSIFIER      ##################\")\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"\\tSMOOTHING\\tACCURACY\\n\\t\\t\\tDEV\\t\\tTEST\")\n",
    "    for smoothing in smoothings:\n",
    "        mnb_classifier(smoothing)\n",
    "        print(\"\\t{}\\t\\t{}\\t{}\".format(smoothing, mnb_results[smoothing][0],mnb_results[smoothing][1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_results = dict()\n",
    "\n",
    "#eval_mnb_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE DL model with Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_classifier(regparam, elasticnetparam):\n",
    "    global lrModel\n",
    "    global lr_results\n",
    "    global train_data, dev_data, test_data\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=regparam, elasticNetParam=elasticnetparam)\n",
    "    #print(\"---------------------------------  Fitting Model to TRAIN  ---------------------------\")\n",
    "    lrModel = lr.fit(train_data)\n",
    "    #print(\"---------------------------------  Generating Predictions  ---------------------------\")\n",
    "    predictions = lrModel.transform(dev_data)\n",
    "    selected = predictions.select(\"label\", \"prediction\", \"userid\", \"country\",\"is_holiday\", \"day_of_week\", \"time_of_day\",\"absence_time\",\"days_since_epoch\",\"genre_cluster\")\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol(\"prediction\")\n",
    "    #print(\"---------------------------------  Evaluating Model     ------------------------------\")\n",
    "    dev_result  = evaluator.evaluate(lrModel.transform(dev_data))\n",
    "    test_result = evaluator.evaluate(lrModel.transform(test_data))\n",
    "    #result.select(\"features\",\"label\",\"prediction\").show(5,False)\n",
    "    lr_results.update({elasticnetparam:{regparam:[dev_result,test_result]}})\n",
    "    #print(\"MULTINOMIAL LOGISTIC REGRESSION RESULTS:\\t{}\".format(lr_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regparams        = [ 0.0001,  0.001, 0.1, 0.3]\n",
    "elasticnetparams = [0.0001,  0.001, 0.1, 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mlr_classifier():\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"###############################         LOGISTIC REGRESSION         #######################\")\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"\\tREGPARAM\\tELASTICNETPARAM\\t\\tACCURACY\\n\\t\\t\\t\\t\\t\\tDEV\\t\\t\\tTEST\")\n",
    "    for elasticnetparam in elasticnetparams:\n",
    "        for regparam in regparams:\n",
    "            logistic_classifier(regparam, elasticnetparam)\n",
    "            #print(lr_results)\n",
    "            #print(lr_results[elasticnetparam][regparam])\n",
    "            dev,test  = lr_results[elasticnetparam][regparam]\n",
    "            print(\"\\t{}\\t\\t{}\\t\\t\\t{}\\t\\t{}\".format(regparam,elasticnetparam,dev,test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_results = {}\n",
    "eval_mlr_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\t0.0001\t\t0.0001\t\t\t0.886543634124\t\t0.862036525917\t0.001\t\t0.0001\t\t\t0.886580966992\t\t0.862090949378\t0.1\t\t0.0001\t\t\t0.883013011394\t\t0.873092263316\t0.3\t\t0.0001\t\t\t0.868542080365\t\t0.865504077872\t0.0001\t\t0.001\t\t\t0.886547189635\t\t0.862036525917\t0.001\t\t0.001\t\t\t0.893503547511\t\t0.827158862083\t0.1\t\t0.001\t\t\t0.882447685095\t\t0.870378865038\t0.3\t\t0.001\t\t\t0.868991852546\t\t0.87603113022\t0.0001\t\t0.1\t\t\t0.854085371382\t\t0.809867750989\t0.001\t\t0.1\t\t\t0.85304716208\t\t0.847800903429\t0.1\t\t0.1\t\t\t0.873478907818\t\t0.874802714953\t0.3\t\t0.1\t\t\t0.834254507055\t\t0.879164366628\t0.0001\t\t0.3\t\t\t0.854261369192\t\t0.809735579726\t0.001\t\t0.3\t\t\t0.883389895592\t\t0.849550228967\t0.1\t\t0.3\t\t\t0.830195890896\t\t0.872579127825\t0.3\t\t0.3\t\t\t0.65494296071\t\t0.72479610639\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.replace(\"\\t\",\",\").replace(\",,,\",\",\").replace(\",,\",\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    [0.0001,0.0001,0.886543634124,0.862036525917],\n",
    "    [0.001,0.0001,0.886580966992,0.862090949378],\n",
    "    [0.1,0.0001,0.883013011394,0.873092263316],\n",
    "    [0.3,0.0001,0.868542080365,0.865504077872],\n",
    "    [0.0001,0.001,0.886547189635,0.862036525917],\n",
    "    [0.001,0.001,0.893503547511,0.827158862083],\n",
    "    [0.1,0.001,0.882447685095,0.870378865038],\n",
    "    [0.3,0.001,0.868991852546,0.87603113022],\n",
    "    [0.0001,0.1,0.854085371382,0.809867750989],\n",
    "    [0.001,0.1,0.85304716208,0.847800903429],\n",
    "    [0.1,0.1,0.873478907818,0.874802714953],\n",
    "    [0.3,0.1,0.834254507055,0.879164366628],\n",
    "    [0.0001,0.3,0.854261369192,0.809735579726],\n",
    "    [0.001,0.3,0.883389895592,0.849550228967],\n",
    "    [0.1,0.3,0.830195890896,0.872579127825],\n",
    "    [0.3,0.3,0.65494296071,0.72479610639]]\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['regparam','elasticparam','dev_data','test_data'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_error  = [1-x for x in results_df['dev_data'].tolist()][::-1]\n",
    "test_error = [1-x for x in results_df['test_data'].tolist()][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = [x for x in range(0,16)]\n",
    "ax.plot(x, dev_error, lw=3, label='Dev')\n",
    "ax.plot(x, test_error, lw=3, label='Test')\n",
    "\n",
    "\n",
    "ax.grid(True)\n",
    "ax.legend(frameon=False)\n",
    "plt.xlabel('Iteration')\n",
    "plt.xlabel('Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\"\"\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "plt.plot(objectiveHistory)\n",
    "plt.ylabel('Objective Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(dev_data)\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('userid','country','time_of_day','absence_time','is_holiday','genre_cluster','prediction').sample(0.001,100).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_temp  = predictions.select(\"label\").groupBy(\"label\").count().sort('count', ascending=False).toPandas()\n",
    "class_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = class_temp['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Create Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions.select('label','prediction')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\tLABEL\\tLabels\\tPredictions\")\n",
    "for label in class_names:\n",
    "    num_labels = df[df['label'] == label].count()\n",
    "    num_correct_preds = df[(df['label'] == label) & (df['prediction'] == label)].count()\n",
    "    print(\"\\t{}\\t{}\\t{}\".format(label,num_labels,num_correct_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rdd = predictions.select('prediction','label').rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(predictions_rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = \\t\\t%s\" % precision)\n",
    "print(\"Recall = \\t\\t%s\" % recall)\n",
    "print(\"F1 Score = \\t\\t%s\" % f1Score)\n",
    "\n",
    "# Statistics by class\n",
    "#labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "labels = class_names\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = \\t%s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = \\t%s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = \\t%s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "# Weighted stats\n",
    "print(\"Weighted recall = \\t%s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = \\t%s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = \\t%s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = metrics.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, DenseMatrix \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set() \n",
    "\n",
    "\n",
    "def display_cm(m):\n",
    "  a = m.toArray().astype(np.float)\n",
    "  row_sums = a.sum(axis=1)\n",
    "  percentage_matrix = a.astype(np.float) / row_sums[:, np.newaxis]\n",
    "  print(percentage_matrix)\n",
    "  plt.figure(figsize=(3, 3))\n",
    "  sns.heatmap(percentage_matrix, annot=True,  fmt='.2f', xticklabels=['0' ,'1','2'], yticklabels=['0' ,'1','2']);\n",
    "  plt.title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cm(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE DL model with an generic MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_classifier():\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"##################################       MLP CLASSIFIER      ##############################\")\n",
    "    print(\"###########################################################################################\")\n",
    "    global mlp_results\n",
    "    global train_data, dev_data, test_data\n",
    "    layer_1 = train_data.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"]\n",
    "    layer_2 = layer_1*4\n",
    "    layer_3 = int(layer_2/2)\n",
    "    layer_4 = num_buckets\n",
    "    layers = [layer_1, layer_2, layer_3, layer_4]\n",
    "    print(\"LAYERS: \\t{}\".format(layers))\n",
    "    print(\"---------------------------------  Training MLP Model        ------------------------------\")\n",
    "    trainer = MultilayerPerceptronClassifier(maxIter=5, layers=layers, blockSize=128)\n",
    "    print(\"---------------------------------  Fitting Model to TRAIN    ------------------------------\")\n",
    "    mlp_model = trainer.fit (train_data)        \n",
    "    print(\"---------------------------------  Testing Model against DEV ------------------------------\")\n",
    "    predictions = mlp_model.transform(dev_data)\n",
    "    selected = predictions.select(\"label\", \"prediction\", \"userid\",\"country\", \"day_of_week\", \"time_of_day\", \"is_holiday\",\n",
    "                                  \"previous_duration\",\"absence_time\",\"is_holiday\",\"genre_cluster\")\n",
    "    print(\"evaluator = MulticlassClassificationEvaluator().....\")\n",
    "    evaluator = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol(\"prediction\")\n",
    "    print(\"result = evaluator.evaluate(mlp_model.transform(dev_data))\")\n",
    "    result = evaluator.evaluate(mlp_model.transform(dev_data))\n",
    "    mlp_results.append(result)\n",
    "    print(\"MLP CLASSIFIER RESULTS:\\t{}\".format(mlp_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_results = []\n",
    "#mlp_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE Model with Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_results = dict()\n",
    "maxBins = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTree_classifier(max_depth):\n",
    "    global dt_results\n",
    "    global train_data, dev_data, test_data\n",
    "    dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=max_depth, maxBins=100)\n",
    "    dtModel = dt.fit(train_data)\n",
    "    #print(\"numNodes = \", dtModel.numNodes)\n",
    "    #print(\"depth = \", dtModel.depth)\n",
    "    evaluator = MulticlassClassificationEvaluator().setMetricName ('accuracy').setPredictionCol ('prediction').setLabelCol ('label')\n",
    "    #print(\"result = evaluator.evaluate(dfModel.transform(dev_data))\")\n",
    "    result = evaluator.evaluate(dtModel.transform(dev_data))\n",
    "    #print(\"dt_results.append(result)\")\n",
    "    dt_results.update({max_depth:result})\n",
    "    #print(\"DECISION TREE CLASSIFIER RESULTS:\\t{}\".format(dt_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dt_classifier():\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"###############################          DECISION TREE              #######################\")\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"\\tNUM_TREES\\tMAX_TREE_DEPTH\\t\\tACCURACY\")\n",
    "    global dt_results\n",
    "    for max_tree_depth in max_tree_depths:\n",
    "        decisionTree_classifier(max_tree_depth)\n",
    "        print(\"\\t\\t\\t\\t{}\\t\\t{}\".format(max_tree_depth,dt_results[max_tree_depth]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tree_depths = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_results = dict()\n",
    "#eval_dt_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE Model with RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = None\n",
    "def randomForest_classifier(num_tree, max_tree_depth):\n",
    "    global rf_results\n",
    "    global rfModel\n",
    "    global train_data, dev_data, test_data\n",
    "    #rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=20, maxBins=100)\n",
    "    rf = RandomForestClassifier(numTrees=num_tree, featureSubsetStrategy=\"auto\", impurity=\"gini\", labelCol=\"label\", featuresCol=\"features\", maxDepth=max_tree_depth, maxBins=100)\n",
    "    \n",
    "    #print(\"---------------------------------  Fitting Model to Train     ------------------------------\")\n",
    "    rfModel = rf.fit(train_data)\n",
    "    #print(\"---------------------------------  Transforming Dev           ------------------------------\")\n",
    "    predictions = rfModel.transform(dev_data)\n",
    "    selected = predictions.select(\"label\", \"prediction\", \"userid\", \"is_holiday\",\"country\", \"day_of_week\", \"time_of_day\",\n",
    "                                  \"absence_time\",\"days_since_epoch\",\"genre_cluster\")\n",
    "    #print(\"---------------------------------     Evaluating Model        ------------------------------\")\n",
    "    evaluator   = MulticlassClassificationEvaluator().setMetricName('accuracy').setPredictionCol ('prediction').setLabelCol ('label')\n",
    "    dev_result  = evaluator.evaluate(rfModel.transform(dev_data))\n",
    "    test_result = evaluator.evaluate(rfModel.transform(test_data))\n",
    "    #print(\"---------------------------------  Summarizing Results        ------------------------------\")\n",
    "    rf_results.update({num_tree:{max_tree_depth:[dev_result, test_result]}})\n",
    "    #print(\"RANDOM FOREST CLASSIFIER RESULTS:\\t{}\".format(rf_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_rf_classifier():\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"###############################          RANDOM FOREST              #######################\")\n",
    "    print(\"###########################################################################################\")\n",
    "    print(\"\\tNUM_TREES\\tMAX_TREE_DEPTH\\t\\tACCURACY\\n\\t\\t\\t\\t\\t\\tDEV\\t\\t\\tTEST\")\n",
    "    for num_tree in num_trees:\n",
    "        for max_tree_depth in max_tree_depths:\n",
    "            randomForest_classifier(num_tree, max_tree_depth)\n",
    "            dev,test = rf_results[num_tree][max_tree_depth]\n",
    "            print(\"\\t{}\\t\\t{}\\t\\t\\t{}\\t\\t{}\".format(num_tree, max_tree_depth,dev,test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tree_depths = [ 5, 10, 15, 20, 30]\n",
    "num_trees       = [ 5, 10, 15, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = dict()\n",
    "\n",
    "eval_rf_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deciles = 25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "deciles = 25\n",
    "\n",
    "###########################################################################################\n",
    "###############################          RANDOM FOREST              #######################\n",
    "###########################################################################################\n",
    "\tNUM_TREES\tMAX_TREE_DEPTH\t\tACCURACY\n",
    "\t\t\t\t\t\tDEV\t\t\tTEST\n",
    "\t5\t\t5\t\t\t0.674507161689\t\t0.688425684764\n",
    "\t5\t\t10\t\t\t0.800491371663\t\t0.74468399406\n",
    "\t5\t\t15\t\t\t0.851774289031\t\t0.831116225189\n",
    "\t5\t\t20\t\t\t0.880554375323\t\t0.860660389827\n",
    "\t5\t\t30\t\t\t0.934310150807\t\t0.950482425109\n",
    "\t10\t\t5\t\t\t0.708462294691\t\t0.796448480419\n",
    "\t10\t\t10\t\t\t0.891016467351\t\t0.846829055908\n",
    "\t10\t\t15\t\t\t0.929309324151\t\t0.899736434952\n",
    "\t10\t\t20\t\t\t0.960526713445\t\t0.947543558206\n",
    "\t10\t\t30\t\t\t0.991724547428\t\t0.997356574743\n",
    "\t15\t\t5\t\t\t0.859086198038\t\t0.797062688052\n",
    "\t15\t\t10\t\t\t0.909874899335\t\t0.869982351249\n",
    "\t15\t\t15\t\t\t0.933277274772\t\t0.904121410967\n",
    "\t15\t\t20\t\t\t0.971532798703\t\t0.962813226456\n",
    "\t15\t\t30\t\t\t0.993955630774\t\t0.998997053358\n",
    "\t20\t\t5\t\t\t0.833157631816\t\t0.78075897404\n",
    "\t20\t\t10\t\t\t0.909764678484\t\t0.865675123036\n",
    "\t20\t\t15\t\t\t0.944830908771\t\t0.921956756673\n",
    "\t20\t\t20\t\t\t0.978757597683\t\t0.974630892312\n",
    "\t20\t\t30\t\t\t0.99360541291\t\t0.99584049261\n",
    "\t30\t\t5\t\t\t0.850375195331\t\t0.780440208053\n",
    "\t30\t\t10\t\t\t0.91116554994\t\t0.864104617442\n",
    "\t30\t\t15\t\t\t0.941101177408\t\t0.901174769283\n",
    "\t30\t\t20\t\t\t0.975973632328\t\t0.969110798392\n",
    "\t30\t\t30\t\t\t0.996748484908\t\t0.999758981815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCIKIT LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SciKitLearn\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso, MultiTaskLassoCV, LassoLarsCV, LassoLarsIC \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV as CV\n",
    "\n",
    "# Other packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import concatenate\n",
    "from pandas import DataFrame, Series, read_csv, scatter_matrix\n",
    "\n",
    "# Install packages\n",
    "!pip install keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Mean Absolute Error Function\n",
    "\n",
    "def get_mean_abs_error(y_true, y_pred, weights):\n",
    "  return np.exp(np.mean(np.mean(np.abs(y_true - y_pred), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_session_with_window_bucketized_descr_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"user_000055\".split(\"_\")[1].lstrip(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_num(userid):\n",
    "    return int(userid.split(\"_\")[1].lstrip(\"0\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_num_udf = udf(lambda userid: get_user_num(userid), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df_ = userSession_df.withColumn('usernum', get_user_num_udf('userid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df_.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_ids = the_model_df_.select('country','country_index').distinct().toPandas().set_index('country').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df_.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(country_ids['Turkey'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_id(country):\n",
    "    try:\n",
    "        return int(country_ids[country][0])\n",
    "    except:\n",
    "        return 9999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_country_id_udf = udf(lambda country: get_country_id(country), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df_ = userSession_df_.withColumn('country_id', get_country_id_udf('country'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSession_df_.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = userSession_df_.select('usernum','session_id','day_of_week_index','time_of_day_index','session_length_1','session_length_2','session_length_3','session_length_4','session_length_5',\"absence_time\",'days_since_epoch','is_holiday','session_length').dropna().toPandas()\n",
    "data = userSession_df_.select('usernum','session_id','day_of_week_index','time_of_day_index',\"genre_10_freq\",\"genre_9_freq\",\"genre_8_freq\",\"genre_7_freq\",\"genre_6_freq\",\"genre_5_freq\",\"genre_4_freq\",\"genre_3_freq\",\"genre_2_freq\",\"genre_1_freq\",\"absence_time\",'days_since_epoch','is_holiday','session_length').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_df = data[[\"userid\",\"age\",\"gender\",\"country_index\", \"day_of_week\", \"time_of_day\",'session_length_1','session_length_2','session_length_3','session_length_4','session_length_5','session_length',\"is_holiday\",\"session_length\"]]\n",
    "model_df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spark = spark.createDataFrame(model_df)\n",
    "model_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_length_bucketizer = QuantileDiscretizer(numBuckets=num_buckets, inputCol=\"session_length\", outputCol=\"decile\", relativeError=0.01, handleInvalid=\"error\")\n",
    "bucketizer = session_length_bucketizer.fit(model_spark)\n",
    "model_bucketized_df = bucketizer.setHandleInvalid(\"skip\").transform(model_spark).drop('session_length').withColumnRenamed('decile','session_length')\n",
    "model_bucketized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_model_df = model_bucketized_df.select('usernum','session_id','day_of_week_index','time_of_day_index',\"genre_10_freq\",\"genre_9_freq\",\"genre_8_freq\",\"genre_7_freq\",\"genre_6_freq\",\"genre_5_freq\",\"genre_4_freq\",\"genre_3_freq\",\"genre_2_freq\",\"genre_1_freq\",\"absence_time\",'days_since_epoch','is_holiday',\"session_length\").toPandas().dropna()\n",
    "unscaled_model_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(unscaled_model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_df = scaler.fit_transform(unscaled_model_df)\n",
    "model_df = pd.DataFrame(scaled_df, columns=['usernum','session_id','day_of_week_index','time_of_day_index',\"genre_10_freq\",\"genre_9_freq\",\"genre_8_freq\",\"genre_7_freq\",\"genre_6_freq\",\"genre_5_freq\",\"genre_4_freq\",\"genre_3_freq\",\"genre_2_freq\",\"genre_1_freq\",\"absence_time\",'days_since_epoch',\"is_holiday\",\"session_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train cut point ~ 65% of samples\n",
    "train_size = int(0.65*model_df.shape[0])\n",
    "\n",
    "#dev cut point ~20% of samples\n",
    "dev_size   = int(0.2*model_df.shape[0])\n",
    "\n",
    "#test cut point ~15% of samples\n",
    "test_size  = int(0.15%model_df.shape[0])\n",
    "test_cut   = train_size + dev_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_       = shuffle(model_df.reset_index())\n",
    "input_data   = np.array(input_.iloc[:,1:-1]).astype(np.float32)\n",
    "input_labels = np.array(input_.iloc[:,-1]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deciles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = input_data[:train_size]\n",
    "train_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = input_labels[:train_size]\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X = input_data[train_size:test_cut]\n",
    "dev_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_Y = input_labels[train_size:test_cut]\n",
    "dev_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = train_X.shape[0]\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_X.shape[1]\n",
    "num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.01\n",
    "training_epochs = 100\n",
    "batch_size      = 1000\n",
    "display_step    = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph Input\n",
    "X = tf.placeholder(tf.float32, [batch_size, num_features])\n",
    "Y = tf.placeholder(tf.float32, [batch_size,])\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([num_features, batch_size]))\n",
    "b = tf.Variable(tf.zeros([batch_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('prediction'):\n",
    "    pred = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "    tf.summary.histogram('predictions', pred)\n",
    "\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(pred), reduction_indices=1))\n",
    "    tf.summary.histogram('cost', cost)\n",
    "    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(i, b_size,data,labels):\n",
    "    start = i*b_size\n",
    "    end = start + b_size\n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ type(x) for x in get_next_batch(0,batch_size, train_X, train_Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir=/data_data/session_length/logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Classifier: Train classifier to predict decile that a session will fall into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0\n",
    "            total_batch = int(n_samples/batch_size)\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                #print(\"ITERATION:\\t{}\\tBATCH_SIZE:\\t{}\".format(i,batch_size))\n",
    "                batch_xs, batch_ys = get_next_batch(i,batch_size, train_X, train_Y)\n",
    "\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "                #print(\"cost:\\t{}\".format(c))\n",
    "                avg_cost += c / total_batch\n",
    "\n",
    "            if (epoch+1) % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "                tf.summary.scalar('average_cost', avg_cost)\n",
    "            \n",
    "        print(\"Optimization Finished!\")\n",
    "        \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(\"Accuracy:\\t{}\".format(accuracy))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    devX = dev_X[:batch_size]\n",
    "    devY = dev_Y[:batch_size]\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy.eval({X: devX, Y: devY}))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('/data_data/session_length/logs/train', sess.graph)\n",
    "    test_writer  = tf.summary.FileWriter('/data_data/session_length/logs/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!whoami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Classifier: Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu', input_shape=(len(train_Y,))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Classifier Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 100\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 7 \n",
    "timesteps = 28 \n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = 100\n",
    "\n",
    "num_classes       = 10\n",
    "\n",
    "_seq_len          = n_samples\n",
    "\n",
    "_labels           = deciles\n",
    "\n",
    "time_steps        = 10\n",
    "\n",
    "embed_dimension   = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs   = tf.placeholder(tf.int32,   shape=[batch_size, time_steps])\n",
    "_labels   = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_size     = 10\n",
    "_seqlens  = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable( tf.random_uniform([_size, embed_dimension], -1.0, 1.0), name='embedding')\n",
    "    embed      = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('lstm'):\n",
    "    \n",
    "    lstm_cell       = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    \n",
    "    print(\"LSTM output_size: \\t{}\\tinput_spec:\\{}\".format(lstm_cell.output_size,lstm_cell.input_spec))\n",
    "    \n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed, sequence_length = _seqlens, dtype=tf.float32)\n",
    "    \n",
    "    weights         = { 'linear_layer': tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean=0, stddev=0.01)) }\n",
    "    \n",
    "    biases          = { 'linear_layer': tf.Variable(tf.truncated_normal([num_classes], mean=0, stddev=0.01))}\n",
    "    \n",
    "    final_output    = tf.matmul(states[1], weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "    \n",
    "    softmax         = tf.nn.softmax_cross_entropy_with_logits(logits = final_output, labels = _labels)\n",
    "    \n",
    "    cross_entropy   = tf.reduce_mean(softmax)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "correction_prediction = tf.equal(tf.argmax(_labels, 1), tf.argmax(final_output, 1))\n",
    "\n",
    "accuracy   = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1000):\n",
    "        \n",
    "        x_batch, y_batch = get_next_batch(step, batch_size, train_X, train_Y)\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "\n",
    "            acc = sess.run(accuracy,feed_dict={inputs:x_batch, _labels:y_batch})\n",
    "\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "            \n",
    "            \n",
    "    for test_batch in range(5):\n",
    "        \n",
    "        x_tes, y_test = get_next_batch(test_batch, batch_size, train_X, train_Y)\n",
    "        \n",
    "        batch_pred, batch_acc = sess.run([tf.argmax(final_output,1), accuracy], feed_dict={_inputs:x_test, _labels:y_test})\n",
    "        \n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "\n",
    "    ouput_example  = session.run([outputs], feed_dict={_inputs:x_test, _labels:y_test})\n",
    "    \n",
    "    states_example = sess.run([states[1]], feed_dict={_inputs:x_test, _labels:y_test})\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
